#!/bin/bash

cleanup() {
    echo "Exitingâ€¦" >&2
    exit 0
}
trap 'cleanup' SIGINT SIGTERM SIGQUIT SIGHUP

declare -A requested_fields
args=('query' '{job="accelerator_logs"}')


# Print the column headers for results in table format
print_headers() {
    printf "%-20s %-15s %-20s %-20s %-20s %-40s\n" "Timestamp" "Accelerator" "Origin" "Facility" "Proc" "Text"
    echo "==========================================================================================================="
}

# Take the output of a logcli command and format it for display as a table. Works for both
# one-off queries and tailing logs
format_as_table() {
    while read -r line; do
        timestamp=$(echo "$line" | awk -F'T| ' '{print $1 " " $2}' | cut -d- -f1,2,3 --output-delimiter='-')

        # Extract the JSON from the line
        if [[ "$line" =~ "accelerator_logs" ]]; then
            json_part=$(echo "$line" | awk -F'{job="accelerator_logs"} ' '{print $2}' | tr -d '\000-\037')
        elif [[ "$line" =~ "accelerator_dev_logs" ]]; then
            json_part=$(echo "$line" | awk -F'{job="accelerator_dev_logs"} ' '{print $2}' | tr -d '\000-\037')
        else
            json_part=$(echo "$line" | awk -F'{} ' '{print $2}' | tr -d '\000-\037')
        fi

        tsv=$(echo "$json_part" | jq -r '[.accelerator, .origin, .facility, .proc, .text] | @tsv')
        accelerator=$(cut -f1  <<<"$tsv")
        origin=$(cut -f2  <<<"$tsv")
        facility=$(cut -f3  <<<"$tsv")
        proc=$(cut -f4  <<<"$tsv")
        text=$(cut -f5- <<<"$tsv")

        printf "%-20s %-15s %-20s %-20s %-20s %-40s\n" "$timestamp" "$accelerator" "$origin" "$facility" "$proc" "$text"
    done
}

# Users are allowed to specify relative dates when setting from and to, this function will translate
# them into the format that logcli expects
process_date() {
    local input_date=$1
    local timezone_offset=$(date +%:z)

    if [[ "$input_date" =~ -[0-9]+[smhdwMy]$ ]]; then
        local date_in_hours=$(convert_to_hours "$input_date")
        local value="${date_in_hours%?}"
        date -d "$value hours" "+%Y-%m-%dT%H:%M:%S.%N$timezone_offset"
    else
        # Use the provided date directly if it doesn't match the above pattern.
        echo "$input_date"
    fi
}

# Converts the input time value to the number of hours in it
convert_to_hours() {
    local input=$1
    local value="${input%[smhdwMy]}" # Extract the numeric part
    local unit="${input: -1}"

    case "$unit" in
        s) echo "$((value / 3600))h" ;;
        m) echo "$((value / 60))h" ;;
        h) echo "${value}h" ;;
        d) echo "$((value * 24))h" ;;
        w) echo "$((value * 24 * 7))h" ;;
        M) echo "$((value * 24 * 30))h" ;;
        y) echo "$((value * 24 * 365))h" ;;
        *) echo "Invalid unit: $unit"; return 1 ;;
    esac
}

# Format the output as json that includes timestamps
convert_json() {
  while IFS= read -r input_json; do
    if [[ -n "$input_json" ]]; then
      echo "${input_json}" | jq '
      {
          accelerator: .line | fromjson.accelerator,
          origin: .line | fromjson.origin,
          user: .line | fromjson.user,
          facility: .line | fromjson.facility,
          severity: .line | fromjson.severity,
          text: .line | fromjson.text,
          timestamp: (.timestamp | sub("[-+][0-9]{2}:[0-9]{2}$"; ""))
      }'
    fi
  done
}

# Add a filter to the query based on user selected fields (e.g., --accelerator)
accelerators=()
add_field() {
    local key=$1
    local value=$2
    if [[ "$key" == "accelerator" ]]; then
        accelerators+=("$value")
    fi
    if [[ -n "${requested_fields[$key]}" ]]; then
        requested_fields[$key]+="|$value"
    else
        requested_fields[$key]="$value"
    fi
}

# Print a warning to the user if the number of results returned is likely higher than the limit that they set
warn_if_limit_hit() {
    if [[ $tail_mode == true ]]; then
        cat
        return
    fi

    local count=0

    while IFS= read -r line; do
        echo "$line"
        if [[ "$line" == *'"accelerator":'* ]]; then
            ((count++))
        fi
    done

    if [[ "$count" -ge "$limit" ]]; then
        print_limit_warning
    fi
}

print_limit_warning() {
    printf "\n\nWARNING: More than %s entries match. Printing chronologically final %s. Use --limit to increase this number.\n\n" "$limit" "$limit" >&2
}

# Condense repeated log lines into a single line, with a short note stating how many lines
# were compacted
compact_logs() {
    if [[ $disable_like == true ]]; then
        cat
        return
    fi

    local prev_text=""
    local prev_line=""
    local count=1

    while read -r line; do
        json_part=$(echo "$line" | awk -F'{} ' '{print $2}' | tr -d '\000-\037')
        current_text=$(echo "$json_part" | jq -r '.text' 2>/dev/null)
        current_origin=$(echo "$json_part" | jq -r '.origin' 2>/dev/null)
        current_facility=$(echo "$json_part" | jq -r '.facility' 2>/dev/null)

        if [[ "$current_text" == "$prev_text" && "$current_origin" == "$prev_origin" && "$current_facility" == "$prev_facility" ]]; then
            ((count++))
        else
            if [[ $count -gt 1 ]]; then
                echo ""
                echo "$prev_line"
                echo "$count Like:"
                echo ""
                count=1
            elif [ ! -z "$prev_line" ]; then
                echo "$prev_line"
            fi
        fi

        prev_text="$current_text"
        prev_origin="$current_origin"
        prev_facility="$current_facility"
        prev_line="$line"
    done

    # For the last set of lines
    if [[ $count -gt 1 ]]; then
        echo "$count Like: $prev_line"
    else
        echo "$prev_line"
    fi
}

# Apply filters specified by the user in their command before making the logcli call
build_query() {
    # If dev logs were requested, that is a different Loki job label than the production ones,
    # so account for that here
    local job_filter=""

    if [[ ${#accelerators[@]} -eq 1 && "${accelerators[0]}" == "LCLS-DEV" ]]; then
        job_filter='accelerator_dev_logs'
    elif [[ " ${accelerators[*]} " =~ " LCLS-DEV " ]]; then
        job_filter='accelerator_logs|accelerator_dev_logs'
    else
        job_filter='accelerator_logs'
    fi

    # Set the updated job label in the query
    args[1]="{job=~\"$job_filter\"}"

    for key in "${!requested_fields[@]}"; do
        value="${requested_fields[$key]}"
        # Multiple values need to not use an exact match, so use '|~'
        if [[ "$value" == *"|"* ]]; then
            args[1]="${args[1]} |~ \"\\\"$key\\\": \\\"(${value})\\\"\""
        else
            args[1]="${args[1]} |= \"\\\"$key\\\": \\\"$value\\\"\""
        fi
    done
}

usage() {
    echo "qlog is a wrapper script around Loki's logcli to facilitate message log queries at SLAC."
    echo " "
    echo "Usage: `basename $0` [OPTIONS]"
    echo "    Default: Returns the most recent 30 log entries excluding change log, watcher, and put logs"
    echo " "
    echo "    Options:"
    echo "    -h | --help: print this usage info and exit"
    echo " "
    echo "    -q | --quiet: Suppress query metadata"
    echo " "
    echo "    -o | --output=default: Specify output mode [default, raw, json, jsonl]. raw suppresses log labels and timestamp. json sends to jq "
    echo " "
    echo "    -a | --accelerator: Specify accelerator to return results from"
    echo "                        Currently supported: LCLS, FACET, TESTFAC, LCLS-DEV"
    echo "                        If not set, defaults to use LCLS and FACET and TESTFAC"
    echo "    -u | --user: Specify the user "
    echo "    -s | --severity: Specify the severity to return results from"
    echo "    --origin: Specify the origin/host "
    echo "    --facility: Specify the facility to return results from"
    echo " "
    echo "    -r | --regex: Return results the match the specified regular expression"
    echo "    -e | --exclude: Return results that do NOT match the specified regular expression"
    echo " "
    echo "    --since=1h: Lookback period. Valid units: s, m, h, d, w, M, y"
    echo "    --from: The earliest date from which to return from"
    echo "            Uses RFC3339Nano or relative time ago from now (s, m, h, d, w, M, y)"
    echo "            --from -10d --to -9d  # between 10 and 9 days ago"
    echo "            --from -36h --to -24h # between 36 and 24 hours ago"
    echo "            --from -10h # omitting --to implies now, so from 10 hours ago to now"
    echo "            --from 2023-09-20T10:00:00-08:00 --to 2023-09-21T10:00:00-08:00"
    echo "    --to: The latest date from which to return results"
    echo "          Uses RFC3339Nano or relative time ago from now, same as --from"
    echo " "
    echo "    --changelog: Include results from change logs"
    echo "    --putlog: Include results from put logging"
    echo "    --watcher: Include results from the watcher"
    echo "    --disable-like: Don't compact multiple repeated log entries into a single \"# Like:\" line"
    echo " "
    echo "    --limit=100 Number of lines of output to return"
    echo "    --table: Print all returned results in a table format"
    echo "    -t | --tail: Tail results to watch logs in real time"
    echo ""
    echo "Example Queries:"
    echo " "
    echo "    Get last 20 lines from FACET over the past 24 hours that include the string KLYS output as a table:"
    echo "        qlog -a FACET -r KLYS --since 24h --limit 20 --table"
    echo ""
    echo "    Get up to 100 (default amount) lines from any accelerator that include the word error (case insensitive) and exclude the word buffer (case sensitive) from yesterday:"
    echo "        qlog -r \"(?i)error\" -e buffer --from -2d --to -1d --table"
    echo ""
    echo "    Tail the logs from either LCLS or LCLS-DEV receiving only those from the CRYO or rf facilities"
    echo "        qlog -a LCLS -a LCLS-DEV --facility CRYO --facility rf --tail"

    exit 0
}


invert=false
changelog=false
watcher=false
putlog=false
disable_like=false
table=false
regex=""
format_json=false
tail_mode=false
limit=100

while [[ $# -gt 0 ]]; do
    case "$1" in
        -h | --help) usage; break; shift ;;
        -i | --invert) invert=true; shift ;;
        --changelog) changelog=true; shift ;;
        --putlog) putlog=true; shift ;;
        --watcher) watcher=true; shift ;;
        --disable-like) disable_like=true; shift ;;
        --table) table=true; shift ;;
        -a | --accelerator) add_field "accelerator" "$2"; shift 2 ;;
        --origin) add_field "origin" "$2"; shift 2 ;;
        -u | --user) add_field "user" "$2"; shift 2 ;;
        --facility) add_field "facility" "$2"; shift 2 ;;
        -s | --severity) add_field "severity" "$2"; shift 2 ;;
        -r | --regex) args[1]="${args[1]} |~ \"$2\""; shift 2 ;;
        -e | --exclude) args[1]="${args[1]} !~ \"$2\""; shift 2 ;;
        --since)
            since_in_hours=$(convert_to_hours "$2")
            args+=("--since=$since_in_hours")
            shift 2
            ;;
        --from)
            numeric_value="$2"
            [[ "$numeric_value" != -* ]] && numeric_value="-$numeric_value"
            processed_from=$(process_date "$numeric_value")
            args+=("--from=$processed_from")
            shift 2
            ;;
        --to)
            numeric_value="$2"
            [[ "$numeric_value" != -* ]] && numeric_value="-$numeric_value"
            processed_to=$(process_date "$numeric_value")
            args+=("--to=$processed_to")
            shift 2
            ;;
        --limit)
            limit="$2"
            args+=("--limit=$limit")
            shift 2
            ;;
        -t|--tail)
            invert=true
            tail_mode=true
            args+=("$1")
            shift
            ;;
        -o|--output)
            if [[ $2 == "json" ]]; then
                format_json=true
                shift 2
            else
                invert=true
                args+=("$1")
                shift
            fi
            ;;
        *) args+=("$1"); shift ;;  # Support logcli options as well
    esac
done

# Set default limit to 100 if user didn't specify one
if ! printf '%s\n' "${args[@]}" | grep -q -- '--limit='; then
    args+=("--limit=$limit")
fi

build_query

# Filter out results from change log, watcher, and put logs unless specifically
# requested by the user
if [[ $changelog == false ]]; then
    args[1]="${args[1]} !~ \"([A-Z]{2,4}:[^ ]+ changed from)\""
fi

if [[ $watcher == false ]]; then
    args[1]="${args[1]} !~ \"(F2:WATCHER)\""
fi

if [[ $putlog == false ]]; then
    args[1]="${args[1]} !~ \"new=[^ ]+ old=\""
fi


# If tail mode is active and user hasn't specified a --since, add --since=0s
# logcli default behavior is to retrieve the last couple minutes of data which doesn't seem
# very useful, especially when we are handling immediate reconnects here
if [[ $tail_mode == true ]]; then
    if ! printf '%s\n' "${args[@]}" | grep -q -- '--since='; then
        args+=("--since=0s")
    fi
fi

# The while loop here handles reconnecting. Loki will only allow tailing logs for a max duration
# of 1 hour, before printing this and forcing a quit:
# Error reading stream: websocket: close 1011 (internal server error): reached tail max duration limit
# Get around it here by just reconnecting until the user (or system) exits the script
# See also: https://github.com/grafana/loki/issues/5086
if [[ $tail_mode == true ]]; then
    # Print headers once if in table mode
    if [[ $table == true ]]; then
        print_headers
    fi

    while true; do
        if [[ $table == true ]]; then
            logcli "${args[@]}" --quiet \
              | format_as_table
        else
            if [[ $format_json == true ]]; then
                logcli "${args[@]}" -o jsonl \
                  | convert_json
            else
                logcli "${args[@]}"
            fi
        fi
        printf "\nReconnecting...\n" >&2
        sleep 1
	printf "\nReconnect successful\n" >&2
    done

    exit 0
fi

# Format output as requested by user. If invert was set to true, assume normal logcli formatting
if [[ $table == true ]]; then
    print_headers
    if [[ $invert == false ]]; then
        logcli "${args[@]}" --quiet | warn_if_limit_hit | compact_logs | format_as_table | tac
    else
        logcli "${args[@]}" --quiet | warn_if_limit_hit | format_as_table
    fi
else
    if [[ $invert == false ]]; then
       if [[ $format_json == true ]]; then
           logcli "${args[@]}" -o jsonl | warn_if_limit_hit | tac | convert_json
       else
           logcli "${args[@]}" | warn_if_limit_hit | compact_logs | tac
       fi
    else
       if [[ $format_json == true ]]; then
           logcli "${args[@]}" -o jsonl | warn_if_limit_hit | convert_json
       else
          logcli "${args[@]}"
       fi
    fi
fi
